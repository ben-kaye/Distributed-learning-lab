{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "from typing import Iterable, List\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "from typing import Any\n",
    "import lightning as L\n",
    "import torch.utils.data as tud\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dims, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(a, b) for a, b in zip(dims[:-1], dims[1:])\n",
    "        ])\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        final = self.linears[-1]\n",
    "        return final(x)\n",
    "\n",
    "\n",
    "class ResMlp(Mlp):\n",
    "    def __init__(self, dims, *args, **kwargs) -> None:\n",
    "        super().__init__(dims, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.act(x)\n",
    "        for layer in self.linears[1:-1]:\n",
    "            z = layer(x)\n",
    "            x = self.act(z) + x\n",
    "\n",
    "        final = self.linears[-1]\n",
    "        return final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self, n_layers, hidden_dim, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp = ResMlp([28*28] + self.n_layers * [self.hidden_dim]+[10])\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_index, *args: Any, **kwargs: Any):\n",
    "        # return super().training_step(*args, **kwargs)\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 784)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "        labels = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, *args):\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 784)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        top1_error = (torch.argmax(logits, -1) != y).sum()/y.shape[0]\n",
    "        self.log('test/top1_error', top1_error)\n",
    "\n",
    "        labels = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log('test/cce_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def on_train_start(self) -> None:\n",
    "        # return super().on_train_start()\n",
    "        self.init_params = {k:v.detach().clone() for k,v in self.state_dict().items() if 'weight' in k and v.shape[0]==v.shape[1] }\n",
    "    def on_train_end(self) -> None:\n",
    "        # return super().on_train_end()\n",
    "        keys = list(self.init_params.keys())\n",
    "        params = self.state_dict()\n",
    "        diffs = [torch.abs(params[k] - self.init_params[k]).flatten().tolist() for k in keys]\n",
    "\n",
    "        _, bins = np.histogram(diffs, bins=10)\n",
    "        hists = [np.histogram(c, bins)[0] for c in diffs]\n",
    "\n",
    "        # wandb.log({\n",
    "        #     'changes/weight_delta': wandb.Table(keys, list(zip(*diffs)))\n",
    "        # }\n",
    "        # )\n",
    "\n",
    "        self.store = diffs\n",
    "        for k, h in enumerate(hists):\n",
    "            wandb.log({\n",
    "                'changes/weights': wandb.Histogram(np_histogram=(h, bins)), 'layer': k\n",
    "            })\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optim = opt.AdamW(self.mlp.parameters())\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4241, -0.8525,  0.2553,  0.0081,  0.2327,  0.4389,  0.3660,  0.4610,\n",
       "          0.7931, -0.3177]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = ResMlp([28*28, 1, 10])\n",
    "\n",
    "X = torch.zeros((1, 28*28))\n",
    "dummy_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MnistData(tud.Dataset):\n",
    "    def __init__(self, train):\n",
    "        self.mnist = MNIST('mnist', train=train)\n",
    "        self.transform = PILToTensor()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, label = self.mnist[index]\n",
    "        return  (self.transform(im)/255).type(torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory ./lightning_logs/z5r4omma/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | mlp  | ResMlp | 4.6 K \n",
      "--------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 201/235 [00:07<00:01, 27.90it/s, v_num=omma]"
     ]
    }
   ],
   "source": [
    "model = Model(20, 5)\n",
    "model.compile()\n",
    "\n",
    "data = tud.DataLoader(MnistData(True), batch_size=256)\n",
    "test_data = tud.DataLoader(MnistData(False), batch_size=1024)\n",
    "\n",
    "logger = L.pytorch.loggers.WandbLogger()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2, logger=logger)\n",
    "logger.watch(model)\n",
    "\n",
    "trainer.fit(model, data, test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 30.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/cce_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7030673027038574     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/top1_error      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20430000126361847    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/cce_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7030673027038574    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/top1_error     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20430000126361847   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/top1_error': 0.20430000126361847, 'test/cce_loss': 0.7030673027038574}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = model.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins = np.histogram(diffs, bins=10)\n",
    "hists = [np.histogram(c, bins)[0] for c in diffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9, 9, 4, 2, 0, 0, 1, 0, 0, 0]),\n",
       " array([6, 6, 6, 3, 3, 1, 0, 0, 0, 0]),\n",
       " array([10,  7,  2,  2,  4,  0,  0,  0,  0,  0]),\n",
       " array([8, 9, 4, 3, 1, 0, 0, 0, 0, 0]),\n",
       " array([10, 11,  1,  2,  0,  0,  1,  0,  0,  0]),\n",
       " array([10, 10,  3,  1,  1,  0,  0,  0,  0,  0]),\n",
       " array([9, 9, 1, 2, 3, 1, 0, 0, 0, 0]),\n",
       " array([11,  7,  4,  1,  0,  2,  0,  0,  0,  0]),\n",
       " array([10,  9,  2,  1,  1,  0,  1,  0,  0,  1]),\n",
       " array([9, 8, 4, 2, 2, 0, 0, 0, 0, 0]),\n",
       " array([8, 6, 6, 2, 1, 2, 0, 0, 0, 0]),\n",
       " array([12,  8,  3,  0,  2,  0,  0,  0,  0,  0]),\n",
       " array([10,  4,  1,  2,  3,  1,  2,  1,  1,  0]),\n",
       " array([8, 7, 5, 2, 3, 0, 0, 0, 0, 0]),\n",
       " array([6, 9, 5, 2, 1, 1, 0, 0, 1, 0]),\n",
       " array([12,  2,  3,  4,  1,  1,  1,  0,  1,  0]),\n",
       " array([11,  7,  1,  3,  3,  0,  0,  0,  0,  0]),\n",
       " array([3, 4, 2, 5, 6, 3, 1, 1, 0, 0]),\n",
       " array([6, 5, 6, 2, 1, 3, 1, 1, 0, 0])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
