{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "from typing import Iterable, List\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "from typing import Any\n",
    "import lightning as L\n",
    "import torch.utils.data as tud\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dims, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(a, b) for a, b in zip(dims[:-1], dims[1:])\n",
    "        ])\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        final = self.linears[-1]\n",
    "        return final(x)\n",
    "\n",
    "\n",
    "class ResMlp(Mlp):\n",
    "    def __init__(self, dims, *args, **kwargs) -> None:\n",
    "        super().__init__(dims, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.act(x)\n",
    "        for layer in self.linears[1:-1]:\n",
    "            z = layer(x)\n",
    "            x = self.act(z) + x\n",
    "\n",
    "        final = self.linears[-1]\n",
    "        return final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self, n_layers, hidden_dim, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp = ResMlp([28*28] + self.n_layers * [self.hidden_dim]+[10])\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.logging = {\n",
    "            'grads_time': []\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_index, *args: Any, **kwargs: Any):\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 784)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "        labels = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, *args):\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 784)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        top1_error = (torch.argmax(logits, -1) != y).sum()/y.shape[0]\n",
    "        self.log('test/top1_error', top1_error)\n",
    "\n",
    "        labels = F.one_hot(y, num_classes=10).type(torch.float32)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        self.log('test/cce_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def\n",
    "\n",
    "    # def\n",
    "    def on_after_backward(self) -> None:\n",
    "        # return super().on_after_backward()\n",
    "        # are_grads = any(v.grad is not None for v in self.parameters())\n",
    "        if not (self.global_step % 100):\n",
    "            grad_clones = {k: v.grad.clone() for v, k in zip(\n",
    "                self.parameters(), self.state_dict().keys())}\n",
    "            self.logging['grads_time'].append((self.global_step, grad_clones))\n",
    "\n",
    "    # def on_before_zero_grad(self, optimizer: Optimizer) -> None:\n",
    "\n",
    "    #     are_grads = any(v.grad is not None for v in self.parameters())\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        # return super().on_train_start()\n",
    "        self.init_params = {k: v.detach().clone() for k, v in self.state_dict(\n",
    "        ).items() if 'weight' in k and v.shape[0] == v.shape[1]}\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        # return super().on_train_end()\n",
    "        keys = list(self.init_params.keys())\n",
    "        params = self.state_dict()\n",
    "        diffs = [(k, params[k].detach() - self.init_params[k])\n",
    "                 for k in keys]\n",
    "\n",
    "        self.logging['diffs'] = diffs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optim = opt.AdamW(self.mlp.parameters())\n",
    "        return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1823, -0.4079, -0.9735,  0.0137,  0.4329,  0.2894, -0.3951, -0.7360,\n",
       "          0.1737,  0.9719]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = ResMlp([28*28, 1, 10])\n",
    "\n",
    "X = torch.zeros((1, 28*28))\n",
    "dummy_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MnistData(tud.Dataset):\n",
    "    def __init__(self, train):\n",
    "        self.mnist = MNIST('mnist', train=train)\n",
    "        self.transform = PILToTensor()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, label = self.mnist[index]\n",
    "        return  (self.transform(im)/255).type(torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenkaye\u001b[0m (\u001b[33mox-ben\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240305_134358-kb642o6h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ox-ben/lightning_logs/runs/kb642o6h' target=\"_blank\">sandy-sky-22</a></strong> to <a href='https://wandb.ai/ox-ben/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ox-ben/lightning_logs' target=\"_blank\">https://wandb.ai/ox-ben/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ox-ben/lightning_logs/runs/kb642o6h' target=\"_blank\">https://wandb.ai/ox-ben/lightning_logs/runs/kb642o6h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | mlp  | ResMlp | 4.6 K \n",
      "--------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 235/235 [00:07<00:00, 30.46it/s, v_num=2o6h]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 235/235 [00:07<00:00, 30.37it/s, v_num=2o6h]\n"
     ]
    }
   ],
   "source": [
    "model = Model(20, 5)\n",
    "model.compile()\n",
    "\n",
    "data = tud.DataLoader(MnistData(True), batch_size=256)\n",
    "test_data = tud.DataLoader(MnistData(False), batch_size=1024)\n",
    "\n",
    "logger = L.pytorch.loggers.WandbLogger()\n",
    "# wandb.define_metric('layers', 'layer_k')\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2, logger=logger)\n",
    "# trainer = L.Trainer(fast_dev_run=True, logger=logger)\n",
    "# logger.watch(model)j\n",
    "\n",
    "trainer.fit(model, data, test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 31.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/cce_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8230375051498413     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/top1_error      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.24250000715255737    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/cce_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8230375051498413    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/top1_error     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24250000715255737   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/top1_error': 0.24250000715255737, 'test/cce_loss': 0.8230375051498413}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(model: Model):\n",
    "    times, id_grads_at_t = zip(*model.logging['grads_time'])\n",
    "    ids_grads_dct = {k: [pairs[k] for pairs in id_grads_at_t] for k in id_grads_at_t[0].keys()}\n",
    "    flattened = {k: [a.flatten() for a in v] for k,v in ids_grads_dct.items()}\n",
    "\n",
    "    keys = [f'mlp.linears.{k}.weight' for k in [4, 8, 12, 16]]\n",
    "\n",
    "\n",
    "    k_ims = []\n",
    "    for k in keys:\n",
    "        tensor_list = flattened[k]\n",
    "        im = torch.stack(tensor_list).cpu().numpy()\n",
    "        k_ims.append((k, im))\n",
    "\n",
    "    param_id, diffs = zip(*model.logging['diffs'])\n",
    "    \n",
    "\n",
    "    return k_ims\n",
    "    \n",
    "    \n",
    "\n",
    "k_ims = analyse(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_waterfall(im):\n",
    "    plt.imshow(im)\n",
    "\n",
    "plot_waterfall(k_ims[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
